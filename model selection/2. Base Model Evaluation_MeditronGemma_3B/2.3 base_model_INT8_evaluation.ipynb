{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNEFR9tlopwiFL6LO/F9UOY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ed686c5e6fc749659fa6b5f64d9c8e0e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aaa6e00f14cb4c13b7ac6820bd268105","IPY_MODEL_a716e04009c149b0b6663284e1bd3403","IPY_MODEL_dcb6258bfc1e4090822127882a0aaddf"],"layout":"IPY_MODEL_282275baddb44433987d298b63eddda1"}},"aaa6e00f14cb4c13b7ac6820bd268105":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60c6e0eac33943bc934344cc24ecfab4","placeholder":"​","style":"IPY_MODEL_f0b15ed735754cf090e33900b1a0e07b","value":"Loading checkpoint shards: 100%"}},"a716e04009c149b0b6663284e1bd3403":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1316eced245545c7813645cba39d1da3","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5641192b8dcb4c3597feab6f82362454","value":2}},"dcb6258bfc1e4090822127882a0aaddf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1316c1e25ba486da28e2ffcf9dbc8d2","placeholder":"​","style":"IPY_MODEL_b388ffc2a7af4e42ba5ecb53b30efd42","value":" 2/2 [01:56&lt;00:00, 49.38s/it]"}},"282275baddb44433987d298b63eddda1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60c6e0eac33943bc934344cc24ecfab4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0b15ed735754cf090e33900b1a0e07b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1316eced245545c7813645cba39d1da3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5641192b8dcb4c3597feab6f82362454":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b1316c1e25ba486da28e2ffcf9dbc8d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b388ffc2a7af4e42ba5ecb53b30efd42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["## **0. LOAD LIBRARY**"],"metadata":{"id":"GgLoTHUNjUPm"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pfmJL4XxjaWy","executionInfo":{"status":"ok","timestamp":1761533933833,"user_tz":-480,"elapsed":1242,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}},"outputId":"1f2f8c54-7c1c-4f0b-bd7f-60e301ac6f48"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch, os"],"metadata":{"id":"yFKxVHqwcwNQ","executionInfo":{"status":"ok","timestamp":1761533946174,"user_tz":-480,"elapsed":12331,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## **1. LOAD BASE MODEL - INT8**"],"metadata":{"id":"2cLCV5Ogce7Z"}},{"cell_type":"code","source":["!pip install -U bitsandbytes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QH-WpJ_CeoHs","executionInfo":{"status":"ok","timestamp":1761533955427,"user_tz":-480,"elapsed":9249,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}},"outputId":"03c2c8a0-d407-4b7f-9b8d-b70cf7465924"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n","Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"]}]},{"cell_type":"code","source":["# Path to save model\n","DRIVE_PATH = \"/content/drive/MyDrive/Meditron3-Gemma2-2B\"\n","\n","# Load model & tokenizer from Drive\n","print(\"1. Loading model from Drive...\")\n","tokenizer = AutoTokenizer.from_pretrained(DRIVE_PATH, trust_remote_code=True)\n","model = AutoModelForCausalLM.from_pretrained(\n","    DRIVE_PATH,\n","    load_in_8bit=True,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n",")\n","print(\"2. Model loaded successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["ed686c5e6fc749659fa6b5f64d9c8e0e","aaa6e00f14cb4c13b7ac6820bd268105","a716e04009c149b0b6663284e1bd3403","dcb6258bfc1e4090822127882a0aaddf","282275baddb44433987d298b63eddda1","60c6e0eac33943bc934344cc24ecfab4","f0b15ed735754cf090e33900b1a0e07b","1316eced245545c7813645cba39d1da3","5641192b8dcb4c3597feab6f82362454","b1316c1e25ba486da28e2ffcf9dbc8d2","b388ffc2a7af4e42ba5ecb53b30efd42"]},"collapsed":true,"id":"aYWVjKpocfGe","executionInfo":{"status":"ok","timestamp":1761534108092,"user_tz":-480,"elapsed":152662,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}},"outputId":"04ba519c-d8ba-48b7-926b-13aa256a63cb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["1. Loading model from Drive...\n"]},{"output_type":"stream","name":"stderr","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed686c5e6fc749659fa6b5f64d9c8e0e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["2. Model loaded successfully!\n"]}]},{"cell_type":"markdown","source":["## **2. EVALUATE WITH DIFFERENT DATASETS**"],"metadata":{"id":"MWs0Oc-2dT_8"}},{"cell_type":"code","source":["import json\n","from tqdm import tqdm\n","import os\n","import torch\n","import psutil\n","import time\n","import numpy as np\n","from tqdm import tqdm"],"metadata":{"id":"OVEsVP8whFHn","executionInfo":{"status":"ok","timestamp":1761534108101,"user_tz":-480,"elapsed":4,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# print out sample data\n","UNIFIED_DIR = \"/content/drive/MyDrive/data_source/unified_format\"\n","DATASETS = [\"medqa\", \"emrqa\", \"pubmedqa\", \"medquad\"]\n","SAMPLE_COUNT = 1\n","for ds_name in DATASETS:\n","    print(f\"\\n{'='*20} SAMPLES FOR: {ds_name.upper()} {'='*20}\")\n","    file_path = os.path.join(UNIFIED_DIR, ds_name, \"all.jsonl\")\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        for i, line in enumerate(f):\n","            if i >= SAMPLE_COUNT: break\n","            print(json.dumps(json.loads(line), indent=2, ensure_ascii=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"kUTcZssOcsit","executionInfo":{"status":"ok","timestamp":1761534113670,"user_tz":-480,"elapsed":5566,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}},"outputId":"7e0885a6-8b2e-40cb-fbd7-da1ffe7462fb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==================== SAMPLES FOR: MEDQA ====================\n","{\n","  \"id\": \"A_23-year-old_pregnant_woman_at_22_weeks\",\n","  \"dataset\": \"medqa\",\n","  \"split\": \"all\",\n","  \"context\": null,\n","  \"question\": \"A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\",\n","  \"text\": [\n","    \"Ampicillin\",\n","    \"Ceftriaxone\",\n","    \"Ciprofloxacin\",\n","    \"Doxycycline\",\n","    \"Nitrofurantoin\"\n","  ],\n","  \"encode\": [\n","    \"A\",\n","    \"B\",\n","    \"C\",\n","    \"D\",\n","    \"E\"\n","  ],\n","  \"answer\": \"E\"\n","}\n","\n","==================== SAMPLES FOR: EMRQA ====================\n","{\n","  \"id\": \"The_patient_was_admiWhat_is_her_current_\",\n","  \"dataset\": \"emrqa\",\n","  \"split\": \"all\",\n","  \"context\": \"The patient was admitted on 5/5/2006 with a history of mechanical fall, with the attending physician being Dr. Clemente Armand Bolstad, with a full code status and disposition of Rehabilitation. Medications on Admission included Amiodarone 100 QD, Colace 100 bid, lasix 40mg QD, Glyburide 5mg bid, Plaquenil 200mg bid, Isordil 20mg tid, Lisinopril 20mg QD, Coumadin 5mg 3dys/week, 2.5mg 4dys/week, Norvasc 10mg QD, Neurontin 300mg TID, with APAP prn. An override was added on 10/2/06 by Gerad E. Dancy, PA for POTENTIALLY SERIOUS INTERACTION: AMIODARONE HCL & WARFARIN with the reason for override being monitoring. The patient was rehydrated with IVF and PO's were encouraged, holding Glypizide while in house, Novolog sliding scale was started on 1/2, Low dose NPH 6 units BID was started on 1/2, bridged with lovenox and INR therapeutic 1/2 and restarted on home regimen of 5/2.5mg variable dose. Pain was controlled with TYLENOL (ACETAMINOPHEN) 650 MG PO Q4H PRN Pain, Headache. A CT pelvis showed a right adnexal cyst which will need further characterization by US and outpatient follow up. The patient has an extensive cardiac history and the fall is not likely related to a cardiac issue as it appears mechanical, with no syncope, chest pain, etc. She was diagnosed with an NSTEMI with a small TnI leak, likely demand related in the setting of hypovolemia and the fall. Enzymes trended down. She was dry on admission and rehydrated with IVF, PO's encouraged, and became euvolemic by 1/2. Her JVP was up to 12cm, although it was difficult to gauge her volume status due to TR. She had a prolonged QT on admission, on telemetry, of unclear etiology, possibly starvation. This was monitored on telemetry until ROMI and drugs that confound were avoided. The QTc resolved to low 500s and a DDD pacer was functioning with V-pacing at 60bpm. Additional medications included NATURAL TEARS (ARTIFICIAL TEARS) 2 DROP OU BID, COLACE (DOCUSATE SODIUM) 100 MG PO BID, PLAQUENIL SULFATE (HYDROXYCHLOROQUINE) 200 MG PO BID, ISORDIL (ISOSORBIDE DINITRATE) 20 MG PO TID, LISINOPRIL 20 MG PO DAILY HOLD IF: SBP <110, MILK OF MAGNESIA (MAGNESIUM HYDROXIDE) 30 MILLILITERS PO DAILY PRN Constipation, COUMADIN (WARFARIN SODIUM) 2.5 MG PO QPM, NORVASC (AMLODIPINE) 10 MG PO DAILY HOLD IF: SBP <110, NEURONTIN (GABAPENTIN) 300 MG PO TID, NEXIUM (ESOMEPRAZOLE) 20 MG PO DAILY, MAALOX-TABLETS QUICK DISSOLVE/CHEWABLE 1-2 TAB PO Q6H PRN Upset Stomach, DULCOLAX RECTAL (BISACODYL RECTAL) 10 MG PR DAILY PRN Constipation, CLOTRIMAZOLE 1% TOPICAL TOPICAL TP BID, GLYBURIDE 5 MG PO BID, LASIX (FUROSEMIDE) 20 MG PO DAILY, and corrected pt restarted on lasix 20 qd on d/c. A PT consult was obtained 3/21 and to follow daily at rehab. Labs showed Na 146, CK 3320, CKMB 12.9, Trop 0.23--->0.10, AST 107, Cr 1.2-->1.6. Pain was controlled with TYLENOL (ACETAMINOPHEN) 650 MG PO Q4H PRN Pain, Headache, rehydrated with IVF, po's encouraged, holding Glypizide while in house, Novolog sliding scale was started on 1/2, Low dose NPH 6 units BID was started on 1/2, bridged with lovenox and INR therapeutic 1/2 and restarted on home regimen of 5/2.5mg variable\",\n","  \"question\": \"What is her current dose of lasix ( furosemide )\",\n","  \"text\": [\n","    \"LASIX (FUROSEMIDE) 20 MG PO DAILY,\"\n","  ],\n","  \"encode\": null,\n","  \"answer\": \"LASIX (FUROSEMIDE) 20 MG PO DAILY,\"\n","}\n","\n","==================== SAMPLES FOR: PUBMEDQA ====================\n","{\n","  \"id\": \"21645374\",\n","  \"dataset\": \"pubmedqa\",\n","  \"split\": \"all\",\n","  \"context\": \"Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cells at the center of these areoles and progresses outwards, stopping approximately five cells from the vasculature. The role of mitochondria during PCD has been recognized in animals; however, it has been less studied during PCD in plants.\\nThe following paper elucidates the role of mitochondrial dynamics during developmentally regulated PCD in vivo in A. madagascariensis. A single areole within a window stage leaf (PCD is occurring) was divided into three areas based on the progression of PCD; cells that will not undergo PCD (NPCD), cells in early stages of PCD (EPCD), and cells in late stages of PCD (LPCD). Window stage leaves were stained with the mitochondrial dye MitoTracker Red CMXRos and examined. Mitochondrial dynamics were delineated into four categories (M1-M4) based on characteristics including distribution, motility, and membrane potential (ΔΨm). A TUNEL assay showed fragmented nDNA in a gradient over these mitochondrial stages. Chloroplasts and transvacuolar strands were also examined using live cell imaging. The possible importance of mitochondrial permeability transition pore (PTP) formation during PCD was indirectly examined via in vivo cyclosporine A (CsA) treatment. This treatment resulted in lace plant leaves with a significantly lower number of perforations compared to controls, and that displayed mitochondrial dynamics similar to that of non-PCD cells.\",\n","  \"question\": \"Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\",\n","  \"text\": [\n","    \"yes\",\n","    \"no\",\n","    \"maybe\"\n","  ],\n","  \"encode\": [\n","    \"A\",\n","    \"B\",\n","    \"C\"\n","  ],\n","  \"answer\": \"A\"\n","}\n","\n","==================== SAMPLES FOR: MEDQUAD ====================\n","{\n","  \"id\": \"0000559-1\",\n","  \"dataset\": \"medquad\",\n","  \"split\": \"all\",\n","  \"context\": null,\n","  \"question\": \"What is (are) keratoderma with woolly hair ?\",\n","  \"text\": [\n","    \"Keratoderma with woolly hair is a group of related conditions that affect the skin and hair and in many cases increase the risk of potentially life-threatening heart problems. People with these conditions have hair that is unusually coarse, dry, fine, and tightly curled. In some cases, the hair is also sparse. The woolly hair texture typically affects only scalp hair and is present from birth. Starting early in life, affected individuals also develop palmoplantar keratoderma, a condition that causes skin on the palms of the hands and the soles of the feet to become thick, scaly, and calloused.  Cardiomyopathy, which is a disease of the heart muscle, is a life-threatening health problem that can develop in people with keratoderma with woolly hair. Unlike the other features of this condition, signs and symptoms of cardiomyopathy may not appear until adolescence or later. Complications of cardiomyopathy can include an abnormal heartbeat (arrhythmia), heart failure, and sudden death.  Keratoderma with woolly hair comprises several related conditions with overlapping signs and symptoms. Researchers have recently proposed classifying keratoderma with woolly hair into four types, based on the underlying genetic cause. Type I, also known as Naxos disease, is characterized by palmoplantar keratoderma, woolly hair, and a form of cardiomyopathy called arrhythmogenic right ventricular cardiomyopathy (ARVC). Type II, also known as Carvajal syndrome, has hair and skin abnormalities similar to type I but features a different form of cardiomyopathy, called dilated left ventricular cardiomyopathy. Type III also has signs and symptoms similar to those of type I, including ARVC, although the hair and skin abnormalities are often milder. Type IV is characterized by palmoplantar keratoderma and woolly and sparse hair, as well as abnormal fingernails and toenails. Type IV does not appear to cause cardiomyopathy.\"\n","  ],\n","  \"encode\": null,\n","  \"answer\": \"Keratoderma with woolly hair is a group of related conditions that affect the skin and hair and in many cases increase the risk of potentially life-threatening heart problems. People with these conditions have hair that is unusually coarse, dry, fine, and tightly curled. In some cases, the hair is also sparse. The woolly hair texture typically affects only scalp hair and is present from birth. Starting early in life, affected individuals also develop palmoplantar keratoderma, a condition that causes skin on the palms of the hands and the soles of the feet to become thick, scaly, and calloused.  Cardiomyopathy, which is a disease of the heart muscle, is a life-threatening health problem that can develop in people with keratoderma with woolly hair. Unlike the other features of this condition, signs and symptoms of cardiomyopathy may not appear until adolescence or later. Complications of cardiomyopathy can include an abnormal heartbeat (arrhythmia), heart failure, and sudden death.  Keratoderma with woolly hair comprises several related conditions with overlapping signs and symptoms. Researchers have recently proposed classifying keratoderma with woolly hair into four types, based on the underlying genetic cause. Type I, also known as Naxos disease, is characterized by palmoplantar keratoderma, woolly hair, and a form of cardiomyopathy called arrhythmogenic right ventricular cardiomyopathy (ARVC). Type II, also known as Carvajal syndrome, has hair and skin abnormalities similar to type I but features a different form of cardiomyopathy, called dilated left ventricular cardiomyopathy. Type III also has signs and symptoms similar to those of type I, including ARVC, although the hair and skin abnormalities are often milder. Type IV is characterized by palmoplantar keratoderma and woolly and sparse hair, as well as abnormal fingernails and toenails. Type IV does not appear to cause cardiomyopathy.\"\n","}\n"]}]},{"cell_type":"markdown","source":["### **2.1 EVALUATE WITH DIFFERENT DATASETS (MCQ tasks)**"],"metadata":{"id":"T2ms2VDlkD1l"}},{"cell_type":"code","source":["UNIFIED_DIR = \"/content/drive/MyDrive/data_source/unified_format\"\n","\n","# Dataset medqua\n","ds_medqa = []\n","file_path = os.path.join(UNIFIED_DIR, \"medqa\", \"all.jsonl\")\n","with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        if line.strip():\n","            ds_medqa.append(json.loads(line))\n","\n","ds_pubmedqa = []\n","file_path = os.path.join(UNIFIED_DIR, \"pubmedqa\", \"all.jsonl\")\n","with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        if line.strip():\n","            ds_pubmedqa.append(json.loads(line))"],"metadata":{"id":"2qMfXxQLkECm","executionInfo":{"status":"ok","timestamp":1761481966757,"user_tz":-480,"elapsed":2545,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def generate_batch_answers_mcq(model, tokenizer, questions, options_list, contexts=None, encodes=None, max_new_tokens=30):\n","    \"\"\"\n","    Generate answers for a batch of MCQ questions.\n","    \"\"\"\n","    # Handle missing contexts or encodes\n","    if contexts is None:\n","        contexts = [None] * len(questions)\n","    if encodes is None:\n","        encodes = [None] * len(questions) # Fix error if some items lack 'encode'\n","\n","    prompts = []\n","    for q, opts, ctx, enc in zip(questions, options_list, contexts, encodes):\n","        # Ensure encode is a list (e.g., ['A', 'B', 'C', 'D'])\n","        # If item lacks 'encode', create default encoding\n","        if enc is None:\n","            enc = [chr(65 + i) for i in range(len(opts))] # A, B, C...\n","\n","        options_str = \"\\n\".join([f\"{e}. {t}\" for e, t in zip(enc, opts)])\n","        prompt = f\"\"\"\n","You are a medical reasoning assistant. Read the context and question carefully, then choose the best answer.\n","Context:\n","{ctx or \"No additional context.\"}\n","Question:\n","{q}\n","Options:\n","{options_str}\n","Please respond with only the letter corresponding to the best answer.\n","Answer:\n","\"\"\"\n","        prompts.append(prompt)\n","\n","    # Tokenize the entire batch\n","    inputs = tokenizer(\n","        prompts,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True\n","    ).to(model.device)\n","\n","    # Generate for the entire batch\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=False,\n","            temperature=0.0\n","        )\n","\n","    # Decode the entire batch\n","    decoded_batch = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","    # Extract the answer parts\n","    answer_parts = [decoded.split(\"Answer:\")[-1].strip() for decoded in decoded_batch]\n","    return answer_parts"],"metadata":{"id":"NnS4psRpnGSd","executionInfo":{"status":"ok","timestamp":1761483200861,"user_tz":-480,"elapsed":6,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# --- UPDATED EVALUATION FUNCTION (USES BATCHING) ---\n","def evaluate_model_mcq_task(model, tokenizer, dataset, batch_size=16, max_new_tokens=30): # <--- Added batch_size\n","    correct = 0\n","    total = len(dataset)\n","    latencies = [] # Will store per-sample latency\n","    total_generation_time = 0.0\n","    process = psutil.Process()\n","\n","    if torch.cuda.is_available():\n","        torch.cuda.reset_peak_memory_stats()\n","\n","    # Get model size on disk\n","    model_size_gb = 0.0\n","    model_dir = getattr(model, 'config', None).name_or_path\n","    if model_dir and os.path.isdir(model_dir): # <--- Added safety check\n","        try:\n","            model_size_bytes = sum(os.path.getsize(os.path.join(root, f))\n","                                   for root, _, files in os.walk(model_dir)\n","                                   for f in files)\n","            model_size_gb = model_size_bytes / (1024 ** 3)\n","        except Exception as e:\n","            print(f\"Warning: Could not calculate model size. Error: {e}\")\n","\n","    # Iterate through the dataset in BATCHES\n","    for i in tqdm(range(0, total, batch_size), desc=\"Evaluating Batches\"):\n","        batch_items = dataset[i:i + batch_size]\n","        num_in_batch = len(batch_items)\n","\n","        # Prepare data for the batch\n","        batch_contexts = [item.get(\"context\") for item in batch_items]\n","        batch_questions = [item[\"question\"] for item in batch_items]\n","        batch_options = [item[\"text\"] if \"text\" in item else item[\"options\"] for item in batch_items]\n","        batch_encodes = [item.get(\"encode\") for item in batch_items]\n","        batch_true_answers = [item[\"answer\"].strip().upper() for item in batch_items]\n","\n","        start_time = time.time()\n","\n","        # Generate answers for the entire batch\n","        pred_answers = generate_batch_answers_mcq(\n","            model, tokenizer, batch_questions, batch_options,\n","            contexts=batch_contexts, encodes=batch_encodes,\n","            max_new_tokens=max_new_tokens\n","        )\n","\n","        batch_time = time.time() - start_time\n","        total_generation_time += batch_time\n","        per_sample_latency = batch_time / num_in_batch\n","        latencies.extend([per_sample_latency] * num_in_batch)\n","\n","        # Compare results for the batch\n","        for pred, true_answer in zip(pred_answers, batch_true_answers):\n","            pred_clean = pred.strip().upper()\n","            if pred_clean.startswith(\"ANSWER:\"):\n","                pred_clean = pred_clean.replace(\"ANSWER:\", \"\").strip()\n","            if \".\" in pred_clean:\n","                pred_clean = pred_clean.split(\".\")[0].strip()\n","\n","            if pred_clean == true_answer:\n","                correct += 1\n","\n","    # Get model performance metrics\n","    accuracy = correct / total if total > 0 else 0\n","    avg_latency = np.mean(latencies) if latencies else 0\n","    p95_latency = np.percentile(latencies, 95) if latencies else 0\n","    p99_latency = np.percentile(latencies, 99) if latencies else 0\n","    vram_peak_gb = torch.cuda.max_memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0.0\n","    cpu_ram_gb = process.memory_info().rss / (1024 ** 3)\n","\n","    result = {\n","        \"accuracy\": accuracy,\n","        \"total_time_sec\": total_generation_time, # This is the total generation time\n","        \"avg_latency_sec\": avg_latency,     # Average latency per sample\n","        \"p95_latency_sec\": p95_latency,\n","        \"p99_latency_sec\": p99_latency,\n","        \"vram_peak_gb\": vram_peak_gb,\n","        \"cpu_ram_gb\": cpu_ram_gb,\n","        \"model_size_gb\": model_size_gb,\n","        \"total_samples\": total,\n","        \"batch_size\": batch_size\n","    }\n","\n","    print(f\"\\n--- Evaluation Complete ---\")\n","    print(f\"Batch size: {batch_size}\")\n","    print(f\"Accuracy: {accuracy:.2%} ({correct}/{total})\")\n","    print(f\"Total time: {total_generation_time:.2f}s | Avg latency/sample: {avg_latency:.3f}s | p95: {p95_latency:.3f}s | p99: {p99_latency:.3f}s\")\n","    print(f\"GPU VRAM peak: {vram_peak_gb:.2f} GB | CPU RAM used: {cpu_ram_gb:.2f} GB\")\n","    print(f\"Model size on disk: {model_size_gb:.2f} GB\" if model_size_gb > 0 else \"Model size unknown\")\n","\n","    return result"],"metadata":{"id":"sgN0xl2PnNcC","executionInfo":{"status":"ok","timestamp":1761483201752,"user_tz":-480,"elapsed":2,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["evaluate_model_mcq_task(model, tokenizer, ds_medqa)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v5-rDfa2pg0S","executionInfo":{"status":"ok","timestamp":1761482715342,"user_tz":-480,"elapsed":748574,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}},"outputId":"a7c7f367-d4c4-4be8-e70f-fefee0e98209"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating:   0%|          | 0/1500 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","Evaluating: 100%|██████████| 1500/1500 [12:28<00:00,  2.00it/s]"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 35.73% (536/1500)\n","Total time: 747.44s | Avg latency: 0.50s | p95: 0.36s | p99: 4.75s\n","GPU VRAM peak: 3.15 GB | CPU RAM used: 2.56 GB\n","Model size on disk: 4.91 GB\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 0.35733333333333334,\n"," 'total_time_sec': 747.4382536411285,\n"," 'avg_latency_sec': np.float64(0.4982921690940857),\n"," 'p95_latency_sec': np.float64(0.3616195321083069),\n"," 'p99_latency_sec': np.float64(4.752051243782043),\n"," 'vram_peak_gb': 3.147794723510742,\n"," 'cpu_ram_gb': 2.563060760498047,\n"," 'model_size_gb': 4.90564379375428,\n"," 'total_samples': 1500}"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["evaluate_model_mcq_task(model, tokenizer, ds_pubmedqa)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sy6NQ3kUnQud","executionInfo":{"status":"ok","timestamp":1761483319807,"user_tz":-480,"elapsed":114715,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}},"outputId":"97607fed-1e0e-4cf9-f298-6a4ed60fbe6c"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating Batches:   0%|          | 0/63 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Evaluating Batches: 100%|██████████| 63/63 [01:54<00:00,  1.82s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","--- Evaluation Complete ---\n","Batch size: 16\n","Accuracy: 72.40% (724/1000)\n","Total time: 114.65s | Avg latency/sample: 0.115s | p95: 0.139s | p99: 0.148s\n","GPU VRAM peak: 5.26 GB | CPU RAM used: 2.61 GB\n","Model size on disk: 4.91 GB\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 0.724,\n"," 'total_time_sec': 114.65121674537659,\n"," 'avg_latency_sec': np.float64(0.11465121674537658),\n"," 'p95_latency_sec': np.float64(0.13871151208877563),\n"," 'p99_latency_sec': np.float64(0.14849765598773956),\n"," 'vram_peak_gb': 5.262834548950195,\n"," 'cpu_ram_gb': 2.609943389892578,\n"," 'model_size_gb': 4.90564379375428,\n"," 'total_samples': 1000,\n"," 'batch_size': 16}"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["### **2.2 EVALUATE WITH DIFFERENT DATASETS (QnA tasks)**"],"metadata":{"id":"sP1kszftw01J"}},{"cell_type":"code","source":["# Install dependencies\n","!pip install -q sentence-transformers psutil tqdm\n","!pip install sentence-transformers -q\n","import os, re, time, psutil, torch, numpy as np\n","from tqdm import tqdm\n","from sentence_transformers import SentenceTransformer, util\n","import os\n","import time\n","import torch\n","import psutil\n","import numpy as np\n","from tqdm import tqdm\n","from sentence_transformers import SentenceTransformer, util\n","import re"],"metadata":{"id":"6-wLkNAt9HTD","executionInfo":{"status":"ok","timestamp":1761534138418,"user_tz":-480,"elapsed":24673,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["UNIFIED_DIR = \"/content/drive/MyDrive/data_source/unified_format\"\n","# Dataset emrqa\n","ds_emrqa = []\n","file_path = os.path.join(UNIFIED_DIR, \"emrqa\", \"all.jsonl\")\n","with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        if line.strip():\n","            ds_emrqa.append(json.loads(line))\n","\n","# Dataset medquad\n","ds_medquad = []\n","file_path = os.path.join(UNIFIED_DIR, \"medquad\", \"all.jsonl\")\n","with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        if line.strip():\n","            ds_medquad.append(json.loads(line))"],"metadata":{"id":"xS_P9QcjxHdV","executionInfo":{"status":"ok","timestamp":1761534141589,"user_tz":-480,"elapsed":3167,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Load BioBERT from Drive\n","BioBERT_BDRIVE_PATH = \"/content/drive/MyDrive/BioBERT/bert_embeddings\"\n","bert_model = SentenceTransformer(BioBERT_BDRIVE_PATH)\n","print(\"BioBERT model loaded from Drive.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_FBjKTuL4z91","executionInfo":{"status":"ok","timestamp":1761534166341,"user_tz":-480,"elapsed":24749,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}},"outputId":"391ec172-99d9-4c96-ff9c-f1649175d828"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["BioBERT model loaded from Drive.\n"]}]},{"cell_type":"code","source":["# Helper function: keyword recall\n","def keyword_recall(pred, target):\n","    \"\"\"Compute fraction of keywords in target that appear in pred.\"\"\"\n","    clean = lambda s: re.findall(r'\\w+', s.lower())\n","    pred_words = set(clean(pred))\n","    target_words = set(clean(target))\n","    matched = pred_words & target_words\n","    recall = len(matched) / len(target_words) if target_words else 0\n","    return recall"],"metadata":{"id":"XgJn2D-Y9LIM","executionInfo":{"status":"ok","timestamp":1761534166352,"user_tz":-480,"elapsed":8,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def generate_batch_answers(model, tokenizer, questions, contexts=None, max_new_tokens=100):\n","    if contexts is None:\n","        contexts = [None] * len(questions)\n","\n","    # Create batch prompt\n","    prompts = []\n","    for q, c in zip(questions, contexts):\n","        prompt = f\"\"\"\n","You are a medical reasoning assistant. Read the context and question carefully and answer concisely.\n","Context:\n","{c or \"No additional context.\"}\n","Question:\n","{q}\n","Answer:\n","\"\"\"\n","        prompts.append(prompt)\n","\n","    # Tokenize batch with padding\n","    inputs = tokenizer(\n","        prompts,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True\n","    ).to(model.device)\n","\n","    # Generate output for batch\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=False,\n","            temperature=0.0\n","        )\n","\n","    # Decode for batch\n","    decoded_batch = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","    # Extract answer for each output\n","    answer_parts = [decoded.split(\"Answer:\")[-1].strip() for decoded in decoded_batch]\n","    return answer_parts"],"metadata":{"id":"_FakE2MD9PXs","executionInfo":{"status":"ok","timestamp":1761534166357,"user_tz":-480,"elapsed":9,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def evaluate_short_answer_task(llm_model, tokenizer, dataset, batch_size=48, alpha=0.5, beta=0.5, max_new_tokens=100):\n","    scores = []\n","    latencies = [] # sample latency\n","    total_generation_time = 0.0\n","\n","    process = psutil.Process()\n","    if torch.cuda.is_available():\n","        torch.cuda.reset_peak_memory_stats()\n","\n","    # Model size on disk\n","    model_size_gb = 0.0\n","    model_dir = getattr(llm_model, 'config', None).name_or_path\n","    if model_dir and os.path.isdir(model_dir):\n","        try:\n","            model_size_bytes = sum(os.path.getsize(os.path.join(root, f))\n","                                   for root, _, files in os.walk(model_dir)\n","                                   for f in files)\n","            model_size_gb = model_size_bytes / (1024 ** 3)\n","        except Exception as e:\n","            print(f\"Warning: Could not calculate model size. Error: {e}\")\n","\n","    # processing batch\n","    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Evaluating Batches\"):\n","        batch_items = dataset[i:i + batch_size]\n","        batch_questions = [item[\"question\"] for item in batch_items]\n","        batch_contexts = [item.get(\"context\") for item in batch_items]\n","        batch_true_answers = [item[\"answer\"] for item in batch_items]\n","\n","        num_in_batch = len(batch_items)\n","\n","        start_time = time.time()\n","\n","        # Generate predictions for all batch\n","        pred_answers = generate_batch_answers(\n","            llm_model, tokenizer, batch_questions, batch_contexts, max_new_tokens=max_new_tokens\n","        )\n","\n","        batch_time = time.time() - start_time\n","        total_generation_time += batch_time\n","\n","        # sample avg latency\n","        per_sample_latency = batch_time / num_in_batch\n","        latencies.extend([per_sample_latency] * num_in_batch)\n","\n","        # evaluate each question in batch\n","        for pred_answer, true_answer in zip(pred_answers, batch_true_answers):\n","            # Keyword recall\n","            kr = keyword_recall(pred_answer, true_answer)\n","\n","            # BERT embedding similarity\n","            emb_pred = bert_model.encode(pred_answer, convert_to_tensor=True)\n","            emb_true = bert_model.encode(true_answer, convert_to_tensor=True)\n","            sim = util.cos_sim(emb_pred, emb_true).item()\n","\n","            # Weighted score\n","            score = alpha * kr + beta * sim\n","            scores.append(score)\n","\n","    # Aggregate metrics\n","    avg_score = np.mean(scores)\n","    avg_latency = np.mean(latencies)\n","    p95_latency = np.percentile(latencies, 95)\n","    p99_latency = np.percentile(latencies, 99)\n","    vram_peak_gb = torch.cuda.max_memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0.0\n","    cpu_ram_gb = process.memory_info().rss / (1024 ** 3)\n","\n","    result = {\n","        \"avg_weighted_score\": avg_score,\n","        \"total_time_sec\": total_generation_time,\n","        \"avg_latency_sec\": avg_latency,     # Avg latency per sample\n","        \"p95_latency_sec\": p95_latency,\n","        \"p99_latency_sec\": p99_latency,\n","        \"vram_peak_gb\": vram_peak_gb,\n","        \"cpu_ram_gb\": cpu_ram_gb,\n","        \"model_size_gb\": model_size_gb,\n","        \"total_samples\": len(dataset),\n","        \"batch_size\": batch_size\n","    }\n","\n","    print(f\"\\n--- Evaluation Complete ---\")\n","    print(f\"Batch size: {batch_size}\")\n","    print(f\"Avg weighted score: {avg_score:.2%}\")\n","    print(f\"Total time: {total_generation_time:.2f}s | Avg latency/sample: {avg_latency:.2f}s | p95: {p95_latency:.2f}s | p99: {p99_latency:.2f}s\")\n","    print(f\"GPU VRAM peak: {vram_peak_gb:.2f} GB | CPU RAM used: {cpu_ram_gb:.2f} GB\")\n","    print(f\"Model size on disk: {model_size_gb:.2f} GB\" if model_size_gb > 0 else \"Model size unknown (path not a local dir)\")\n","\n","    return result"],"metadata":{"id":"UMdQeOAE9VRG","executionInfo":{"status":"ok","timestamp":1761534166366,"user_tz":-480,"elapsed":7,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["evaluate_short_answer_task(model, tokenizer, ds_emrqa, alpha=0.5, beta=0.5, max_new_tokens=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fzDjnfEa9dBS","executionInfo":{"status":"ok","timestamp":1761485034576,"user_tz":-480,"elapsed":1464777,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}},"outputId":"3c4b558c-352a-45f3-b9ee-095f8577cdbe"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating Batches: 100%|██████████| 32/32 [24:24<00:00, 45.77s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","--- Evaluation Complete ---\n","Batch size: 48\n","Avg weighted score: 41.87%\n","Total time: 1432.07s | Avg latency/sample: 0.95s | p95: 0.97s | p99: 0.97s\n","GPU VRAM peak: 17.95 GB | CPU RAM used: 2.82 GB\n","Model size on disk: 4.91 GB\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["{'avg_weighted_score': np.float64(0.41873325632585445),\n"," 'total_time_sec': 1432.0712389945984,\n"," 'avg_latency_sec': np.float64(0.9547141593297322),\n"," 'p95_latency_sec': np.float64(0.9653037935495377),\n"," 'p99_latency_sec': np.float64(0.9710039645433426),\n"," 'vram_peak_gb': 17.948999404907227,\n"," 'cpu_ram_gb': 2.8170394897460938,\n"," 'model_size_gb': 4.90564379375428,\n"," 'total_samples': 1500,\n"," 'batch_size': 48}"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["evaluate_short_answer_task(model, tokenizer, ds_medquad, alpha=0.5, beta=0.5, max_new_tokens=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BRzsqmSFMKfc","executionInfo":{"status":"ok","timestamp":1761534923487,"user_tz":-480,"elapsed":757118,"user":{"displayName":"Binh Minh Tran","userId":"10705473691109304515"}},"outputId":"2fee540f-e51f-4cba-c109-a7fb57cad59c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["\rEvaluating Batches:   0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","Evaluating Batches: 100%|██████████| 32/32 [12:37<00:00, 23.66s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","--- Evaluation Complete ---\n","Batch size: 48\n","Avg weighted score: 32.85%\n","Total time: 715.46s | Avg latency/sample: 0.48s | p95: 0.76s | p99: 0.81s\n","GPU VRAM peak: 4.28 GB | CPU RAM used: 2.48 GB\n","Model size on disk: 4.91 GB\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["{'avg_weighted_score': np.float64(0.32851091916811326),\n"," 'total_time_sec': 715.4611971378326,\n"," 'avg_latency_sec': np.float64(0.47697413142522177),\n"," 'p95_latency_sec': np.float64(0.7596260408560435),\n"," 'p99_latency_sec': np.float64(0.8079953293005625),\n"," 'vram_peak_gb': 4.2806077003479,\n"," 'cpu_ram_gb': 2.475482940673828,\n"," 'model_size_gb': 4.90564379375428,\n"," 'total_samples': 1500,\n"," 'batch_size': 48}"]},"metadata":{},"execution_count":13}]}]}