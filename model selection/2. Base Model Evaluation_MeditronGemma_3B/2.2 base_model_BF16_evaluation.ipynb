{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgLoTHUNjUPm"
   },
   "source": [
    "## **0. LOAD LIBRARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21281,
     "status": "ok",
     "timestamp": 1761535051176,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "pfmJL4XxjaWy",
    "outputId": "fab82a35-64e9-4074-a89e-5632505cc481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 13933,
     "status": "ok",
     "timestamp": 1761535065116,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "yFKxVHqwcwNQ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cLCV5Ogce7Z"
   },
   "source": [
    "## **1. LOAD BASE MODEL - BF16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "8fd55ff640fd4fe089988e43961a8932",
      "dbe05c0ee35348f79e24337f61dd1180",
      "120ba3285c724ac1ad38593280559b20",
      "a04ca97c8e9244f38dcaa83b32ddeffd",
      "77381189e2524349949ab4dd82146275",
      "82a1875d272741a6b0d5219924e214c3",
      "74d8e25e6bbb424588bedfba4463a8c9",
      "3a6e2e9cd96446cbbd599ad638030093",
      "544480279f1e49bfa45343c0cd68e0d9",
      "a75dcc7b9383449ab4cd8d9b01fd773f",
      "15c11ccc3a984ab698e962e84eb66655"
     ]
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 92480,
     "status": "ok",
     "timestamp": 1761535157599,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "aYWVjKpocfGe",
    "outputId": "dcb63ea5-8bc0-4cbe-8198-4cdeb61f98a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading model from Drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd55ff640fd4fe089988e43961a8932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Path to save model\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/Meditron3-Gemma2-2B\"\n",
    "\n",
    "# Load model & tokenizer from Drive\n",
    "print(\"1. Loading model from Drive...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DRIVE_PATH, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DRIVE_PATH,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"2. Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWs0Oc-2dT_8"
   },
   "source": [
    "## **2. EVALUATE WITH DIFFERENT DATASETS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1761535162068,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "OVEsVP8whFHn"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import psutil\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2677,
     "status": "ok",
     "timestamp": 1761535166592,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "kUTcZssOcsit",
    "outputId": "49ef8b88-1771-4bd5-e73f-f67bcbce154d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== SAMPLES FOR: MEDQA ====================\n",
      "{\n",
      "  \"id\": \"A_23-year-old_pregnant_woman_at_22_weeks\",\n",
      "  \"dataset\": \"medqa\",\n",
      "  \"split\": \"all\",\n",
      "  \"context\": null,\n",
      "  \"question\": \"A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\",\n",
      "  \"text\": [\n",
      "    \"Ampicillin\",\n",
      "    \"Ceftriaxone\",\n",
      "    \"Ciprofloxacin\",\n",
      "    \"Doxycycline\",\n",
      "    \"Nitrofurantoin\"\n",
      "  ],\n",
      "  \"encode\": [\n",
      "    \"A\",\n",
      "    \"B\",\n",
      "    \"C\",\n",
      "    \"D\",\n",
      "    \"E\"\n",
      "  ],\n",
      "  \"answer\": \"E\"\n",
      "}\n",
      "\n",
      "==================== SAMPLES FOR: EMRQA ====================\n",
      "{\n",
      "  \"id\": \"The_patient_was_admiWhat_is_her_current_\",\n",
      "  \"dataset\": \"emrqa\",\n",
      "  \"split\": \"all\",\n",
      "  \"context\": \"The patient was admitted on 5/5/2006 with a history of mechanical fall, with the attending physician being Dr. Clemente Armand Bolstad, with a full code status and disposition of Rehabilitation. Medications on Admission included Amiodarone 100 QD, Colace 100 bid, lasix 40mg QD, Glyburide 5mg bid, Plaquenil 200mg bid, Isordil 20mg tid, Lisinopril 20mg QD, Coumadin 5mg 3dys/week, 2.5mg 4dys/week, Norvasc 10mg QD, Neurontin 300mg TID, with APAP prn. An override was added on 10/2/06 by Gerad E. Dancy, PA for POTENTIALLY SERIOUS INTERACTION: AMIODARONE HCL & WARFARIN with the reason for override being monitoring. The patient was rehydrated with IVF and PO's were encouraged, holding Glypizide while in house, Novolog sliding scale was started on 1/2, Low dose NPH 6 units BID was started on 1/2, bridged with lovenox and INR therapeutic 1/2 and restarted on home regimen of 5/2.5mg variable dose. Pain was controlled with TYLENOL (ACETAMINOPHEN) 650 MG PO Q4H PRN Pain, Headache. A CT pelvis showed a right adnexal cyst which will need further characterization by US and outpatient follow up. The patient has an extensive cardiac history and the fall is not likely related to a cardiac issue as it appears mechanical, with no syncope, chest pain, etc. She was diagnosed with an NSTEMI with a small TnI leak, likely demand related in the setting of hypovolemia and the fall. Enzymes trended down. She was dry on admission and rehydrated with IVF, PO's encouraged, and became euvolemic by 1/2. Her JVP was up to 12cm, although it was difficult to gauge her volume status due to TR. She had a prolonged QT on admission, on telemetry, of unclear etiology, possibly starvation. This was monitored on telemetry until ROMI and drugs that confound were avoided. The QTc resolved to low 500s and a DDD pacer was functioning with V-pacing at 60bpm. Additional medications included NATURAL TEARS (ARTIFICIAL TEARS) 2 DROP OU BID, COLACE (DOCUSATE SODIUM) 100 MG PO BID, PLAQUENIL SULFATE (HYDROXYCHLOROQUINE) 200 MG PO BID, ISORDIL (ISOSORBIDE DINITRATE) 20 MG PO TID, LISINOPRIL 20 MG PO DAILY HOLD IF: SBP <110, MILK OF MAGNESIA (MAGNESIUM HYDROXIDE) 30 MILLILITERS PO DAILY PRN Constipation, COUMADIN (WARFARIN SODIUM) 2.5 MG PO QPM, NORVASC (AMLODIPINE) 10 MG PO DAILY HOLD IF: SBP <110, NEURONTIN (GABAPENTIN) 300 MG PO TID, NEXIUM (ESOMEPRAZOLE) 20 MG PO DAILY, MAALOX-TABLETS QUICK DISSOLVE/CHEWABLE 1-2 TAB PO Q6H PRN Upset Stomach, DULCOLAX RECTAL (BISACODYL RECTAL) 10 MG PR DAILY PRN Constipation, CLOTRIMAZOLE 1% TOPICAL TOPICAL TP BID, GLYBURIDE 5 MG PO BID, LASIX (FUROSEMIDE) 20 MG PO DAILY, and corrected pt restarted on lasix 20 qd on d/c. A PT consult was obtained 3/21 and to follow daily at rehab. Labs showed Na 146, CK 3320, CKMB 12.9, Trop 0.23--->0.10, AST 107, Cr 1.2-->1.6. Pain was controlled with TYLENOL (ACETAMINOPHEN) 650 MG PO Q4H PRN Pain, Headache, rehydrated with IVF, po's encouraged, holding Glypizide while in house, Novolog sliding scale was started on 1/2, Low dose NPH 6 units BID was started on 1/2, bridged with lovenox and INR therapeutic 1/2 and restarted on home regimen of 5/2.5mg variable\",\n",
      "  \"question\": \"What is her current dose of lasix ( furosemide )\",\n",
      "  \"text\": [\n",
      "    \"LASIX (FUROSEMIDE) 20 MG PO DAILY,\"\n",
      "  ],\n",
      "  \"encode\": null,\n",
      "  \"answer\": \"LASIX (FUROSEMIDE) 20 MG PO DAILY,\"\n",
      "}\n",
      "\n",
      "==================== SAMPLES FOR: PUBMEDQA ====================\n",
      "{\n",
      "  \"id\": \"21645374\",\n",
      "  \"dataset\": \"pubmedqa\",\n",
      "  \"split\": \"all\",\n",
      "  \"context\": \"Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cells at the center of these areoles and progresses outwards, stopping approximately five cells from the vasculature. The role of mitochondria during PCD has been recognized in animals; however, it has been less studied during PCD in plants.\\nThe following paper elucidates the role of mitochondrial dynamics during developmentally regulated PCD in vivo in A. madagascariensis. A single areole within a window stage leaf (PCD is occurring) was divided into three areas based on the progression of PCD; cells that will not undergo PCD (NPCD), cells in early stages of PCD (EPCD), and cells in late stages of PCD (LPCD). Window stage leaves were stained with the mitochondrial dye MitoTracker Red CMXRos and examined. Mitochondrial dynamics were delineated into four categories (M1-M4) based on characteristics including distribution, motility, and membrane potential (ΔΨm). A TUNEL assay showed fragmented nDNA in a gradient over these mitochondrial stages. Chloroplasts and transvacuolar strands were also examined using live cell imaging. The possible importance of mitochondrial permeability transition pore (PTP) formation during PCD was indirectly examined via in vivo cyclosporine A (CsA) treatment. This treatment resulted in lace plant leaves with a significantly lower number of perforations compared to controls, and that displayed mitochondrial dynamics similar to that of non-PCD cells.\",\n",
      "  \"question\": \"Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\",\n",
      "  \"text\": [\n",
      "    \"yes\",\n",
      "    \"no\",\n",
      "    \"maybe\"\n",
      "  ],\n",
      "  \"encode\": [\n",
      "    \"A\",\n",
      "    \"B\",\n",
      "    \"C\"\n",
      "  ],\n",
      "  \"answer\": \"A\"\n",
      "}\n",
      "\n",
      "==================== SAMPLES FOR: MEDQUAD ====================\n",
      "{\n",
      "  \"id\": \"0000559-1\",\n",
      "  \"dataset\": \"medquad\",\n",
      "  \"split\": \"all\",\n",
      "  \"context\": null,\n",
      "  \"question\": \"What is (are) keratoderma with woolly hair ?\",\n",
      "  \"text\": [\n",
      "    \"Keratoderma with woolly hair is a group of related conditions that affect the skin and hair and in many cases increase the risk of potentially life-threatening heart problems. People with these conditions have hair that is unusually coarse, dry, fine, and tightly curled. In some cases, the hair is also sparse. The woolly hair texture typically affects only scalp hair and is present from birth. Starting early in life, affected individuals also develop palmoplantar keratoderma, a condition that causes skin on the palms of the hands and the soles of the feet to become thick, scaly, and calloused.  Cardiomyopathy, which is a disease of the heart muscle, is a life-threatening health problem that can develop in people with keratoderma with woolly hair. Unlike the other features of this condition, signs and symptoms of cardiomyopathy may not appear until adolescence or later. Complications of cardiomyopathy can include an abnormal heartbeat (arrhythmia), heart failure, and sudden death.  Keratoderma with woolly hair comprises several related conditions with overlapping signs and symptoms. Researchers have recently proposed classifying keratoderma with woolly hair into four types, based on the underlying genetic cause. Type I, also known as Naxos disease, is characterized by palmoplantar keratoderma, woolly hair, and a form of cardiomyopathy called arrhythmogenic right ventricular cardiomyopathy (ARVC). Type II, also known as Carvajal syndrome, has hair and skin abnormalities similar to type I but features a different form of cardiomyopathy, called dilated left ventricular cardiomyopathy. Type III also has signs and symptoms similar to those of type I, including ARVC, although the hair and skin abnormalities are often milder. Type IV is characterized by palmoplantar keratoderma and woolly and sparse hair, as well as abnormal fingernails and toenails. Type IV does not appear to cause cardiomyopathy.\"\n",
      "  ],\n",
      "  \"encode\": null,\n",
      "  \"answer\": \"Keratoderma with woolly hair is a group of related conditions that affect the skin and hair and in many cases increase the risk of potentially life-threatening heart problems. People with these conditions have hair that is unusually coarse, dry, fine, and tightly curled. In some cases, the hair is also sparse. The woolly hair texture typically affects only scalp hair and is present from birth. Starting early in life, affected individuals also develop palmoplantar keratoderma, a condition that causes skin on the palms of the hands and the soles of the feet to become thick, scaly, and calloused.  Cardiomyopathy, which is a disease of the heart muscle, is a life-threatening health problem that can develop in people with keratoderma with woolly hair. Unlike the other features of this condition, signs and symptoms of cardiomyopathy may not appear until adolescence or later. Complications of cardiomyopathy can include an abnormal heartbeat (arrhythmia), heart failure, and sudden death.  Keratoderma with woolly hair comprises several related conditions with overlapping signs and symptoms. Researchers have recently proposed classifying keratoderma with woolly hair into four types, based on the underlying genetic cause. Type I, also known as Naxos disease, is characterized by palmoplantar keratoderma, woolly hair, and a form of cardiomyopathy called arrhythmogenic right ventricular cardiomyopathy (ARVC). Type II, also known as Carvajal syndrome, has hair and skin abnormalities similar to type I but features a different form of cardiomyopathy, called dilated left ventricular cardiomyopathy. Type III also has signs and symptoms similar to those of type I, including ARVC, although the hair and skin abnormalities are often milder. Type IV is characterized by palmoplantar keratoderma and woolly and sparse hair, as well as abnormal fingernails and toenails. Type IV does not appear to cause cardiomyopathy.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# print out sample data\n",
    "UNIFIED_DIR = \"/content/drive/MyDrive/data_source/unified_format\"\n",
    "DATASETS = [\"medqa\", \"emrqa\", \"pubmedqa\", \"medquad\"]\n",
    "SAMPLE_COUNT = 1\n",
    "for ds_name in DATASETS:\n",
    "    print(f\"\\n{'='*20} SAMPLES FOR: {ds_name.upper()} {'='*20}\")\n",
    "    file_path = os.path.join(UNIFIED_DIR, ds_name, \"all.jsonl\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= SAMPLE_COUNT: break\n",
    "            print(json.dumps(json.loads(line), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ms2VDlkD1l"
   },
   "source": [
    "### **2.1 EVALUATE WITH DIFFERENT DATASETS (MCQ tasks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qMfXxQLkECm"
   },
   "outputs": [],
   "source": [
    "UNIFIED_DIR = \"/content/drive/MyDrive/data_source/unified_format\"\n",
    "\n",
    "# Dataset medqua\n",
    "ds_medqa = []\n",
    "file_path = os.path.join(UNIFIED_DIR, \"medqa\", \"all.jsonl\")\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            ds_medqa.append(json.loads(line))\n",
    "\n",
    "ds_pubmedqa = []\n",
    "file_path = os.path.join(UNIFIED_DIR, \"pubmedqa\", \"all.jsonl\")\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            ds_pubmedqa.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnS4psRpnGSd"
   },
   "outputs": [],
   "source": [
    "# Function to receive MCQ prompt input and generate answer\n",
    "def generate_answer(model, tokenizer, question, options, context=None, encode=None, max_new_tokens=30):\n",
    "    options_str = \"\\n\".join([f\"{e}. {t}\" for e, t in zip(encode, options)])\n",
    "    prompt = f\"\"\"\n",
    "You are a medical reasoning assistant. Read the context and question carefully, then choose the best answer.\n",
    "Context:\n",
    "{context or \"No additional context.\"}\n",
    "Question:\n",
    "{question}\n",
    "Options:\n",
    "{options_str}\n",
    "Please respond with only the letter corresponding to the best answer.\n",
    "Answer:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.0\n",
    "        )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer_part = decoded.split(\"Answer:\")[-1].strip()\n",
    "    return answer_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgN0xl2PnNcC"
   },
   "outputs": [],
   "source": [
    "# Function to measure accuracy with other performance metrics\n",
    "def evaluate_model_mcq_task(model, tokenizer, dataset, max_new_tokens=30):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "    times = []\n",
    "    process = psutil.Process()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Get model size on disk\n",
    "    model_dir = getattr(model, 'config', None).name_or_path\n",
    "    if model_dir is None:\n",
    "        model_dir = \"./model_temp_dir\"\n",
    "    model_size_bytes = sum(os.path.getsize(os.path.join(root, f))\n",
    "                            for root, _, files in os.walk(model_dir)\n",
    "                            for f in files)\n",
    "    model_size_gb = model_size_bytes / (1024 ** 3)\n",
    "\n",
    "    # Extract information from prompt and feed into function generate answer\n",
    "    for item in tqdm(dataset, desc=\"Evaluating\"):\n",
    "        start_time = time.time()\n",
    "        context = item.get(\"context\")\n",
    "        question = item[\"question\"]\n",
    "        options = item[\"text\"] if \"text\" in item else item[\"options\"]\n",
    "        encode = item.get(\"encode\")\n",
    "        true_answer = item[\"answer\"].strip().upper()\n",
    "        pred = generate_answer(model, tokenizer, question, options, context=context, encode=encode, max_new_tokens=max_new_tokens)\n",
    "        times.append(time.time() - start_time)\n",
    "\n",
    "        # Get model predictions compare with answers\n",
    "        pred_clean = pred.strip().upper()\n",
    "        if pred_clean.startswith(\"ANSWER:\"):\n",
    "            pred_clean = pred_clean.replace(\"ANSWER:\", \"\").strip()\n",
    "        if \".\" in pred_clean:\n",
    "            pred_clean = pred_clean.split(\".\")[0].strip()\n",
    "        if pred_clean == true_answer:\n",
    "            correct += 1\n",
    "\n",
    "    # Get model performance metrics\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    avg_latency = np.mean(times) if times else 0\n",
    "    p95_latency = np.percentile(times, 95) if times else 0\n",
    "    p99_latency = np.percentile(times, 99) if times else 0\n",
    "    total_time = sum(times)\n",
    "    vram_peak_gb = torch.cuda.max_memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0.0\n",
    "    cpu_ram_gb = process.memory_info().rss / (1024 ** 3)\n",
    "\n",
    "    result = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"total_time_sec\": total_time,\n",
    "        \"avg_latency_sec\": avg_latency,\n",
    "        \"p95_latency_sec\": p95_latency,\n",
    "        \"p99_latency_sec\": p99_latency,\n",
    "        \"vram_peak_gb\": vram_peak_gb,\n",
    "        \"cpu_ram_gb\": cpu_ram_gb,\n",
    "        \"model_size_gb\": model_size_gb,\n",
    "        \"total_samples\": total\n",
    "    }\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    print(f\"Total time: {total_time:.2f}s | Avg latency: {avg_latency:.2f}s | p95: {p95_latency:.2f}s | p99: {p99_latency:.2f}s\")\n",
    "    print(f\"GPU VRAM peak: {vram_peak_gb:.2f} GB | CPU RAM used: {cpu_ram_gb:.2f} GB\")\n",
    "    print(f\"Model size on disk: {model_size_gb:.2f} GB\" if model_size_gb else \"Model size unknown\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150480,
     "status": "ok",
     "timestamp": 1761469244988,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "v5-rDfa2pg0S",
    "outputId": "bb12325c-f2ab-43d8-a099-4e8bfe35ced7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1500/1500 [02:30<00:00,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.27% (559/1500)\n",
      "Total time: 149.91s | Avg latency: 0.10s | p95: 0.09s | p99: 0.12s\n",
      "GPU VRAM peak: 5.01 GB | CPU RAM used: 2.35 GB\n",
      "Model size on disk: 4.91 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.37266666666666665,\n",
       " 'total_time_sec': 149.9098961353302,\n",
       " 'avg_latency_sec': np.float64(0.0999399307568868),\n",
       " 'p95_latency_sec': np.float64(0.0947158694267273),\n",
       " 'p99_latency_sec': np.float64(0.11509149789810179),\n",
       " 'vram_peak_gb': 5.010180950164795,\n",
       " 'cpu_ram_gb': 2.3490371704101562,\n",
       " 'model_size_gb': 4.90564379375428,\n",
       " 'total_samples': 1500}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model_mcq_task(model, tokenizer, ds_medqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94856,
     "status": "ok",
     "timestamp": 1761469339853,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "Sy6NQ3kUnQud",
    "outputId": "488e3d6a-63da-4f36-977f-7fc1b93c5451"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [01:34<00:00, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.30% (743/1000)\n",
      "Total time: 94.47s | Avg latency: 0.09s | p95: 0.11s | p99: 0.12s\n",
      "GPU VRAM peak: 5.01 GB | CPU RAM used: 2.35 GB\n",
      "Model size on disk: 4.91 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.743,\n",
       " 'total_time_sec': 94.47320938110352,\n",
       " 'avg_latency_sec': np.float64(0.09447320938110351),\n",
       " 'p95_latency_sec': np.float64(0.11222063302993775),\n",
       " 'p99_latency_sec': np.float64(0.11685961961746215),\n",
       " 'vram_peak_gb': 5.006504058837891,\n",
       " 'cpu_ram_gb': 2.3490562438964844,\n",
       " 'model_size_gb': 4.90564379375428,\n",
       " 'total_samples': 1000}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model_mcq_task(model, tokenizer, ds_pubmedqa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sP1kszftw01J"
   },
   "source": [
    "### **2.2 EVALUATE WITH DIFFERENT DATASETS (QnA tasks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 14771,
     "status": "ok",
     "timestamp": 1761535223853,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "6-wLkNAt9HTD"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q sentence-transformers psutil tqdm\n",
    "!pip install sentence-transformers -q\n",
    "import os, re, time, psutil, torch, numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2371,
     "status": "ok",
     "timestamp": 1761535226229,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "xS_P9QcjxHdV"
   },
   "outputs": [],
   "source": [
    "UNIFIED_DIR = \"/content/drive/MyDrive/data_source/unified_format\"\n",
    "# Dataset emrqa\n",
    "ds_emrqa = []\n",
    "file_path = os.path.join(UNIFIED_DIR, \"emrqa\", \"all.jsonl\")\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            ds_emrqa.append(json.loads(line))\n",
    "\n",
    "# Dataset medquad\n",
    "ds_medquad = []\n",
    "file_path = os.path.join(UNIFIED_DIR, \"medquad\", \"all.jsonl\")\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            ds_medquad.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14894,
     "status": "ok",
     "timestamp": 1761535241127,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "_FBjKTuL4z91",
    "outputId": "ac880f86-be82-4d2f-b140-a9ef77a8ddb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioBERT model loaded from Drive.\n"
     ]
    }
   ],
   "source": [
    "# Load BioBERT from Drive\n",
    "BioBERT_BDRIVE_PATH = \"/content/drive/MyDrive/BioBERT/bert_embeddings\"\n",
    "bert_model = SentenceTransformer(BioBERT_BDRIVE_PATH)\n",
    "print(\"BioBERT model loaded from Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1761535241133,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "XgJn2D-Y9LIM"
   },
   "outputs": [],
   "source": [
    "# Helper function: keyword recall\n",
    "def keyword_recall(pred, target):\n",
    "    \"\"\"Compute fraction of keywords in target that appear in pred.\"\"\"\n",
    "    clean = lambda s: re.findall(r'\\w+', s.lower())\n",
    "    pred_words = set(clean(pred))\n",
    "    target_words = set(clean(target))\n",
    "    matched = pred_words & target_words\n",
    "    recall = len(matched) / len(target_words) if target_words else 0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1761535241141,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "_FakE2MD9PXs"
   },
   "outputs": [],
   "source": [
    "def generate_batch_answers(model, tokenizer, questions, contexts=None, max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Generate answers for a batch of questions and contexts.\n",
    "    \"\"\"\n",
    "    if contexts is None:\n",
    "        contexts = [None] * len(questions)\n",
    "\n",
    "    # Create batch prompt\n",
    "    prompts = []\n",
    "    for q, c in zip(questions, contexts):\n",
    "        prompt = f\"\"\"\n",
    "You are a medical reasoning assistant. Read the context and question carefully and answer concisely.\n",
    "Context:\n",
    "{c or \"No additional context.\"}\n",
    "Question:\n",
    "{q}\n",
    "Answer:\n",
    "\"\"\"\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    # Tokenize batch with padding\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate output for batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "    # Decode for batch\n",
    "    decoded_batch = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Extract answer for each output\n",
    "    answer_parts = [decoded.split(\"Answer:\")[-1].strip() for decoded in decoded_batch]\n",
    "    return answer_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1761535241147,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "UMdQeOAE9VRG"
   },
   "outputs": [],
   "source": [
    "def evaluate_short_answer_task(llm_model, tokenizer, dataset, batch_size=32, alpha=0.5, beta=0.5, max_new_tokens=100):\n",
    "    scores = []\n",
    "    latencies = [] # sample latency\n",
    "    total_generation_time = 0.0\n",
    "\n",
    "    process = psutil.Process()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Model size on disk\n",
    "    model_size_gb = 0.0\n",
    "    model_dir = getattr(llm_model, 'config', None).name_or_path\n",
    "    if model_dir and os.path.isdir(model_dir):\n",
    "        try:\n",
    "            model_size_bytes = sum(os.path.getsize(os.path.join(root, f))\n",
    "                                   for root, _, files in os.walk(model_dir)\n",
    "                                   for f in files)\n",
    "            model_size_gb = model_size_bytes / (1024 ** 3)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not calculate model size. Error: {e}\")\n",
    "\n",
    "    # processing batch\n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Evaluating Batches\"):\n",
    "        batch_items = dataset[i:i + batch_size]\n",
    "        batch_questions = [item[\"question\"] for item in batch_items]\n",
    "        batch_contexts = [item.get(\"context\") for item in batch_items]\n",
    "        batch_true_answers = [item[\"answer\"] for item in batch_items]\n",
    "\n",
    "        num_in_batch = len(batch_items)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Generate predictions for all batch\n",
    "        pred_answers = generate_batch_answers(\n",
    "            llm_model, tokenizer, batch_questions, batch_contexts, max_new_tokens=max_new_tokens\n",
    "        )\n",
    "\n",
    "        batch_time = time.time() - start_time\n",
    "        total_generation_time += batch_time\n",
    "\n",
    "        # sample avg latency\n",
    "        per_sample_latency = batch_time / num_in_batch\n",
    "        latencies.extend([per_sample_latency] * num_in_batch)\n",
    "\n",
    "        # evaluate each question in batch\n",
    "        for pred_answer, true_answer in zip(pred_answers, batch_true_answers):\n",
    "            # Keyword recall\n",
    "            kr = keyword_recall(pred_answer, true_answer)\n",
    "\n",
    "            # BERT embedding similarity\n",
    "            emb_pred = bert_model.encode(pred_answer, convert_to_tensor=True)\n",
    "            emb_true = bert_model.encode(true_answer, convert_to_tensor=True)\n",
    "            sim = util.cos_sim(emb_pred, emb_true).item()\n",
    "\n",
    "            # Weighted score\n",
    "            score = alpha * kr + beta * sim\n",
    "            scores.append(score)\n",
    "\n",
    "    # Aggregate metrics\n",
    "    avg_score = np.mean(scores)\n",
    "    avg_latency = np.mean(latencies)\n",
    "    p95_latency = np.percentile(latencies, 95)\n",
    "    p99_latency = np.percentile(latencies, 99)\n",
    "    vram_peak_gb = torch.cuda.max_memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0.0\n",
    "    cpu_ram_gb = process.memory_info().rss / (1024 ** 3)\n",
    "\n",
    "    result = {\n",
    "        \"avg_weighted_score\": avg_score,\n",
    "        \"total_time_sec\": total_generation_time,\n",
    "        \"avg_latency_sec\": avg_latency,     # Avg latency per sample\n",
    "        \"p95_latency_sec\": p95_latency,\n",
    "        \"p99_latency_sec\": p99_latency,\n",
    "        \"vram_peak_gb\": vram_peak_gb,\n",
    "        \"cpu_ram_gb\": cpu_ram_gb,\n",
    "        \"model_size_gb\": model_size_gb,\n",
    "        \"total_samples\": len(dataset),\n",
    "        \"batch_size\": batch_size\n",
    "    }\n",
    "\n",
    "    print(f\"\\n--- Evaluation Complete ---\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Avg weighted score: {avg_score:.2%}\")\n",
    "    print(f\"Total time: {total_generation_time:.2f}s | Avg latency/sample: {avg_latency:.2f}s | p95: {p95_latency:.2f}s | p99: {p99_latency:.2f}s\")\n",
    "    print(f\"GPU VRAM peak: {vram_peak_gb:.2f} GB | CPU RAM used: {cpu_ram_gb:.2f} GB\")\n",
    "    print(f\"Model size on disk: {model_size_gb:.2f} GB\" if model_size_gb > 0 else \"Model size unknown (path not a local dir)\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 970798,
     "status": "ok",
     "timestamp": 1761476531167,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "fzDjnfEa9dBS",
    "outputId": "e11c1975-387e-43b8-ba36-73e667f9fd9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Batches: 100%|██████████| 47/47 [16:10<00:00, 20.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Batch size: 32\n",
      "Avg weighted score: 41.03%\n",
      "Total time: 939.97s | Avg latency/sample: 0.63s | p95: 0.64s | p99: 0.66s\n",
      "GPU VRAM peak: 12.36 GB | CPU RAM used: 2.56 GB\n",
      "Model size on disk: 4.91 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_weighted_score': np.float64(0.41030950599151267),\n",
       " 'total_time_sec': 939.9735083580017,\n",
       " 'avg_latency_sec': np.float64(0.6266490055720011),\n",
       " 'p95_latency_sec': np.float64(0.6448302492499352),\n",
       " 'p99_latency_sec': np.float64(0.6552763836724418),\n",
       " 'vram_peak_gb': 12.362613677978516,\n",
       " 'cpu_ram_gb': 2.558429718017578,\n",
       " 'model_size_gb': 4.90564379375428,\n",
       " 'total_samples': 1500,\n",
       " 'batch_size': 32}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_short_answer_task(model, tokenizer, ds_emrqa, alpha=0.5, beta=0.5, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1139717,
     "status": "ok",
     "timestamp": 1761536390749,
     "user": {
      "displayName": "Binh Minh Tran",
      "userId": "10705473691109304515"
     },
     "user_tz": -480
    },
    "id": "BRzsqmSFMKfc",
    "outputId": "77faa1eb-a725-4ce7-f86c-a86b5f10f34f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Batches:   0%|          | 0/47 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating Batches: 100%|██████████| 47/47 [18:59<00:00, 24.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Batch size: 32\n",
      "Avg weighted score: 32.57%\n",
      "Total time: 1094.44s | Avg latency/sample: 0.73s | p95: 0.77s | p99: 0.81s\n",
      "GPU VRAM peak: 5.98 GB | CPU RAM used: 2.12 GB\n",
      "Model size on disk: 4.91 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_weighted_score': np.float64(0.3256809326343647),\n",
       " 'total_time_sec': 1094.4351711273193,\n",
       " 'avg_latency_sec': np.float64(0.729623447418213),\n",
       " 'p95_latency_sec': np.float64(0.7698532342910767),\n",
       " 'p99_latency_sec': np.float64(0.8085334130695888),\n",
       " 'vram_peak_gb': 5.982316017150879,\n",
       " 'cpu_ram_gb': 2.1169509887695312,\n",
       " 'model_size_gb': 4.90564379375428,\n",
       " 'total_samples': 1500,\n",
       " 'batch_size': 32}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_short_answer_task(model, tokenizer, ds_medquad, alpha=0.5, beta=0.5, max_new_tokens=100)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOshI4LT3TcQB+KTXmwkFAr",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "120ba3285c724ac1ad38593280559b20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a6e2e9cd96446cbbd599ad638030093",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_544480279f1e49bfa45343c0cd68e0d9",
      "value": 2
     }
    },
    "15c11ccc3a984ab698e962e84eb66655": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a6e2e9cd96446cbbd599ad638030093": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "544480279f1e49bfa45343c0cd68e0d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "74d8e25e6bbb424588bedfba4463a8c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77381189e2524349949ab4dd82146275": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82a1875d272741a6b0d5219924e214c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fd55ff640fd4fe089988e43961a8932": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dbe05c0ee35348f79e24337f61dd1180",
       "IPY_MODEL_120ba3285c724ac1ad38593280559b20",
       "IPY_MODEL_a04ca97c8e9244f38dcaa83b32ddeffd"
      ],
      "layout": "IPY_MODEL_77381189e2524349949ab4dd82146275"
     }
    },
    "a04ca97c8e9244f38dcaa83b32ddeffd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a75dcc7b9383449ab4cd8d9b01fd773f",
      "placeholder": "​",
      "style": "IPY_MODEL_15c11ccc3a984ab698e962e84eb66655",
      "value": " 2/2 [01:20&lt;00:00, 33.80s/it]"
     }
    },
    "a75dcc7b9383449ab4cd8d9b01fd773f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbe05c0ee35348f79e24337f61dd1180": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82a1875d272741a6b0d5219924e214c3",
      "placeholder": "​",
      "style": "IPY_MODEL_74d8e25e6bbb424588bedfba4463a8c9",
      "value": "Loading checkpoint shards: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
